
## Local Controller Implementation

> Implement a local deployment mode that runs inference engines directly on local GPU without Docker or Kubernetes

<details>
<summary>Implementation Details</summary>

### Overview

Added a new `local` deployment mode that allows running SGLang/vLLM inference servers directly on the local machine without containerization.

### Files Created/Modified

1. **`src/controllers/local_controller.py`** (555 lines) - New controller for local deployment
   - Process management for SGLang/vLLM servers
   - Port allocation (8000-8100 range)
   - Health checking and readiness detection
   - GPU device assignment via CUDA_VISIBLE_DEVICES
   - Graceful shutdown and cleanup

2. **`src/controllers/__init__.py`** - Updated to include LocalController
   - Added import and export for LocalController
   - Updated `get_controller()` factory function

3. **`src/orchestrator.py`** - Added local mode support
   - New `deployment_mode: "local"` option
   - Integration with LocalController for experiment lifecycle

4. **`src/run_autotuner.py`** - CLI support for local mode
   - Added `--mode local` option

5. **`examples/local_task.json`** - Example configuration for local mode

### Key Features

- **No Docker/Kubernetes required**: Direct process spawning
- **GPU allocation**: Automatic CUDA device assignment
- **Port management**: Dynamic port allocation to avoid conflicts
- **Health monitoring**: Polls `/health` endpoint until ready
- **Clean shutdown**: Proper process termination on experiment completion

### Usage

```bash
python src/run_autotuner.py examples/local_task.json --mode local --direct
```

</details>

---

## Frontend UI Updates

> Add local mode option to task creation UI and fix frontend issues

<details>
<summary>Changes</summary>

### Local Mode in UI

- Added "local" option to deployment mode selector in NewTask.tsx
- Users can now create tasks with `deployment_mode: "local"` from the web interface

### Bug Fixes

- Fixed frontend build issues
- Corrected import paths

### Files Modified

- `frontend/src/pages/NewTask.tsx` - Added local mode option
- Various frontend fixes for compatibility

</details>

---

## HuggingFace Offline Mode Fix

> Remove manual HF_HUB_OFFLINE=1 setting that was blocking model downloads

<details>
<summary>Fix Details</summary>

Removed hardcoded `HF_HUB_OFFLINE=1` environment variable that was preventing HuggingFace model/tokenizer downloads during benchmark execution.

This allows genai-bench to properly download tokenizers when needed for benchmarking.

</details>
