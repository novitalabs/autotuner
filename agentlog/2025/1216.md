
## vLLM Runtime Support and Port Conflict Fix

> Add support for vLLM runtime in local controller and fix port conflicts with web server

<details>
<summary>Implementation Details</summary>

### Problem

1. Local controller was hardcoded for SGLang runtime only
2. Port range 8000-8100 conflicted with autotuner web server (port 8000/9002)

### Solution

#### 1. Dynamic Runtime Python Path

Modified orchestrator to support separate Python environments for different runtimes:

```python
# SGLang: uses .venv-sglang
# vLLM: uses /root/work/vllm/env (or configurable path)
```

The runtime's Python interpreter is now dynamically selected based on `base_runtime` in task config.

#### 2. Port Range Change

Changed local controller port allocation from `8000-8100` to `30000-30100` to avoid conflicts:

- Web server uses port 8000 (or 9002 via .env)
- Local inference servers now use ports 30000+

### Files Modified

1. **`src/controllers/local_controller.py`** (+109 lines)
   - Updated port range constants
   - Added vLLM-specific server startup logic
   - Different CLI arguments for SGLang vs vLLM

2. **`src/orchestrator.py`** (+36 lines)
   - Added `get_runtime_python_path()` function
   - Dynamic environment selection based on runtime

### Runtime-Specific Configurations

| Runtime | Python Path | Server Module |
|---------|-------------|---------------|
| SGLang | `.venv-sglang/bin/python` | `sglang.launch_server` |
| vLLM | `/root/work/vllm/env/bin/python` | `vllm.entrypoints.openai.api_server` |

### Testing

Verified both SGLang and vLLM can be launched via local controller without port conflicts.

</details>
