> Implement distributed ARQ worker support with Redis-based registry for worker registration, status tracking, and heartbeat monitoring.

<details>
<summary>Agent Response</summary>

Completed implementation of distributed worker system with full worker lifecycle management:

**Core Components Created:**

1. **Worker Schema** (`src/web/schemas/worker.py`):
   - `GPUInfo`: GPU details (model, count, memory)
   - `WorkerCapabilities`: Worker hardware specs and runtime capabilities
   - `WorkerInfo`: Complete worker registration data including status and timestamps
   - `WorkerRegister`: Registration request payload
   - `WorkerHeartbeat`: Periodic heartbeat update with current metrics

2. **Redis Worker Registry** (`src/web/workers/registry.py`):
   - `WorkerRegistry` class managing all worker lifecycle operations
   - `register_worker()`: Adds worker to registry with TTL
   - `heartbeat()`: Updates worker status and resets TTL
   - `get_workers()`: Retrieves all active workers with filtering
   - Auto-cleanup of expired workers via Redis TTL

3. **Workers API Routes** (`src/web/routes/workers.py`):
   - `GET /api/workers` - List all active workers
   - `POST /api/workers/register` - Register new worker
   - `POST /api/workers/heartbeat` - Send heartbeat and update status
   - Status badges: online/busy/offline with proper error handling

4. **Worker Auto-Registration** (modified `src/web/workers/autotuner_worker.py`):
   - Registers on startup with hostname, GPU info, and capabilities
   - Sends heartbeats every 30 seconds via background task
   - Automatic cleanup on graceful shutdown

5. **Remote Worker Deployment**:
   - `.env.worker.example`: Configuration template for remote workers
   - `scripts/start_remote_worker.sh`: Bash script to start workers with custom manager/Redis addresses

**Technical Resolution:**
- Fixed Pydantic nested model validation by using `ConfigDict(from_attributes=True)` in GPUInfo
- Separated worker status (online/busy/offline) from task execution state
- Redis TTL handles automatic worker timeout (default 90 seconds)

**Worker Visibility:**
Workers now display:
- Worker ID, hostname, status badge
- GPU configuration (model, count, VRAM)
- Heartbeat timestamp and uptime
- Task assignment state (online vs busy)

**Next Steps:**
- Extend frontend Dashboard to display worker status table
- Test with remote RTX4090 worker via `start_remote_worker.sh`
- Implement work distribution logic to assign tasks to available workers

</details>
