> Enhanced worker deployment and restoration system with auto SSH key configuration, offline venv syncing, and intelligent error suggestions

<details>
<summary>Agent Response</summary>

## Worker Restore Improvements

Enhanced the worker deployment/restore system with three major improvements:

### 1. Auto SSH Key Configuration (`setup_ssh_key_for_manager()`)
- Automatically generates SSH key on remote worker if not present
- Adds remote worker's public key to manager's authorized_keys
- Verifies SSH connectivity from worker to manager
- Enables SSH tunnel for Redis access without manual key setup

### 2. Offline venv Sync (`sync_venv_offline()`)
- Syncs local venv's site-packages to remote via rsync/tar
- Bypasses pip install when network is unreachable
- Auto-fallback when online install fails due to network issues
- Fixes venv bin scripts for correct Python path

### 3. Intelligent Error Suggestions (`get_deployment_error_suggestion()`)
- Provides context-specific fix suggestions for common errors
- Handles: SSH key issues, Redis localhost, network unreachable, module not found, project not found, venv issues, timeouts
- Added `suggestion` field to `DeploymentResult` and API response schema

## Files Modified
- `src/web/services/worker_service.py` - Added 3 new helper functions, updated `start_remote_worker()` to use them
- `src/web/schemas/worker.py` - Added `suggestion` field to `WorkerSlotRestoreResponse`
- `src/web/routes/workers.py` - Pass suggestion field through to response

## Testing Status
- Previous session tested worker slots: 196.2, 196.3, bj-7.67, bj-8.71
- 196.2 had SSH key authentication issues (now auto-handled)
- 196.3 had network unreachable issues (now has offline fallback)
- bj-7.67 and bj-8.71 successfully running in OME mode with 32 GPUs each

</details>

<details>
<summary>Remote Worker Log Synchronization</summary>

## Remote Worker Log Synchronization

Implemented real-time log synchronization from remote workers to the central manager server.

### Architecture
```
Worker → AsyncRedisLogHandler → Redis Pub/Sub Channel → Result Listener → WebSocket/API
              │                         │
              └── Queue ─────────────────┘ (circular buffer: last 500 entries)
```

### Implementation Details

#### 1. Worker-side Log Publishing (`src/web/workers/log_handler.py`)
- Custom `AsyncRedisLogHandler` that captures Python logs
- Background thread with async event loop for non-blocking publishing
- Logs queued and published to Redis `channel:logs:{worker_id}`
- Stored in circular buffer `logs:buffer:{worker_id}` (500 entries, 24h TTL)
- Classifies log sources: worker, task, benchmark, deployment

#### 2. Manager-side Log Subscription (`src/web/services/result_listener.py`)
- `ResultListener` subscribes to `channel:logs:*` pattern
- `on_log()` callback registration for log entry processing
- `_dispatch_log()` method dispatches to registered callbacks

#### 3. API Endpoints (`src/web/routes/workers.py`)
- `GET /api/workers/{worker_id}/logs` - Fetch recent logs (max 500)
- Returns JSON with worker_id, level, message, source, timestamp

#### 4. WebSocket Streaming (`src/web/routes/websocket.py`)
- `ws://server/ws/workers/{worker_id}/logs` - Real-time log streaming
- Filters logs by worker_id
- Sends ping every 30s to keep connection alive

### Key Bug Fixes
- Fixed import path issue: Try both `src.web.workers.pubsub` and `web.workers.pubsub`
- Fixed root logger level: Default WARNING level filtered INFO logs
- Added automatic logger level adjustment in `setup_remote_logging()`

### Files Modified
- `src/web/workers/log_handler.py` - NEW: AsyncRedisLogHandler class
- `src/web/workers/pubsub.py` - Added LogEntry model, log channel constants, publish_log method
- `src/web/workers/autotuner_worker.py` - Initialize/shutdown remote logging in on_startup/on_shutdown
- `src/web/services/result_listener.py` - Added log subscription and dispatch
- `src/web/routes/workers.py` - Added GET /{worker_id}/logs endpoint
- `src/web/routes/websocket.py` - Added ws/workers/{worker_id}/logs endpoint

### Testing
- Verified logs stored in Redis: `redis-cli KEYS "logs:buffer:*"`
- Tested API: `curl http://localhost:9002/api/workers/{worker_id}/logs`
- Logs include: remote logging enabled, worker config, worker registration, etc.

</details>

<details>
<summary>Fix Pydantic Serialization Warnings in Worker Heartbeats</summary>

## Pydantic Serialization Warning Fix

Fixed recurring Pydantic serialization warnings that were polluting worker logs during heartbeats.

### Problem
Worker logs showed repeated warnings:
```
PydanticSerializationUnexpectedValue(Expected `GPUInfo` - serialized value may not be as expected
[field_name='gpus', input_value={'index': 0, 'name': 'NVI...}, input_type=dict])
```

### Root Cause
- `WorkerInfo.gpus` was typed as `Optional[List[GPUInfo]]`
- After JSON deserialization from Redis, the data comes back as dicts
- When assigning dicts to the `gpus` field, Pydantic warned about type mismatch during serialization

### Solution
Updated `WorkerInfo` schema to accept both GPUInfo objects and dicts:

```python
# src/web/schemas/worker.py
class WorkerInfo(BaseModel):
    # Use Union to accept both GPUInfo objects and dicts (for Redis storage)
    gpus: Optional[List[Union[GPUInfo, dict]]] = None
    # Use Union to accept both WorkerCapabilities objects and dicts (for Redis storage)
    capabilities: Optional[Union[WorkerCapabilities, dict]] = None
```

### Files Modified
- `src/web/schemas/worker.py` - Added `Union` import, updated type annotations
- `src/web/workers/registry.py` - Kept dict conversion for consistent Redis JSON storage

### Verification
- Worker on 196.3 (`GPU-RTX4090-10-121-196-3-7916`) started cleanly without warnings
- Task 89 was processed and experiment result synced via Redis Pub/Sub
- Backend log confirmed: `✅ Synced experiment 1 to local database`

</details>
